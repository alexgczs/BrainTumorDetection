{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Preprocesado de imagenes\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport os","metadata":{"execution":{"iopub.status.busy":"2022-10-29T23:03:52.278553Z","iopub.execute_input":"2022-10-29T23:03:52.279666Z","iopub.status.idle":"2022-10-29T23:03:52.286865Z","shell.execute_reply.started":"2022-10-29T23:03:52.279610Z","shell.execute_reply":"2022-10-29T23:03:52.285565Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"#1 de todo hay que añadir las carpetas train y test de este link https://www.kaggle.com/datasets/jonathanbesomi/rsna-miccai-png\n# dentro de una carpeta llamada 'data' en el directorio principal del proyecto\n\n#Aqui cargo las fotos del train y del test\n\npath='../input/rsna-miccai-png/train'\n\nimages=[]\npacientes=[]\nlist_directorios=[folder[1] for folder in os.walk(path)]\nfor paciente in list_directorios[0]:\n    fotos_paciente=[folde[2] for folde in os.walk(f'{path}/{paciente}/FLAIR')]\n    if len(fotos_paciente)==0:\n        continue\n    for foto in fotos_paciente[0]:\n        images.append(mpimg.imread(f\"{path}/{paciente}/FLAIR/{foto}\"))\n        pacientes.append(paciente)\n        \npath='../input/rsna-miccai-png/test'\nimages_t=[]\npacientes_t=[]\nlist_directorios=[folder[1] for folder in os.walk(path)]\nfor paciente in list_directorios[0]:\n    fotos_paciente=[folde[2] for folde in os.walk(f'{path}/{paciente}/FLAIR')\n    if len(fotos_paciente)==0:\n        continue\n    for foto in fotos_paciente[0]:\n        images_t.append(mpimg.imread(f\"{path}/{paciente}/FLAIR/{foto}\"))\n        pacientes_t.append(paciente)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-29T23:03:52.534229Z","iopub.execute_input":"2022-10-29T23:03:52.534840Z","iopub.status.idle":"2022-10-29T23:03:52.564229Z","shell.execute_reply.started":"2022-10-29T23:03:52.534786Z","shell.execute_reply":"2022-10-29T23:03:52.562574Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_96/547103752.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpacientes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mlist_directorios\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolder\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpaciente\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist_directorios\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfotos_paciente\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfolde\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfolde\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{path}/{paciente}/FLAIR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"],"ename":"TypeError","evalue":"'generator' object is not subscriptable","output_type":"error"}]},{"cell_type":"code","source":"\npath='../input/rsna-miccai-png/train'\n\nimages=[]\npacientes=[]\nlist_directorios=[folder[1] for folder in os.walk(path)]\nfor paciente in list_directorios[0][:10]:\n    fotos_paciente=[folde[2] for folde in os.walk(f'{path}/{paciente}/FLAIR')]\n    if len(fotos_paciente)==0:\n        continue\n    for foto in fotos_paciente[0]:\n        images.append(mpimg.imread(f\"{path}/{paciente}/FLAIR/{foto}\"))\n        pacientes.append(paciente)\n        \n        \n        \npath='../input/rsna-miccai-png/test'\nimages_t=[]\npacientes_t=[]\nlist_directorios=[folder[1] for folder in os.walk(path)]\nfor paciente in list_directorios[0][:10]:\n    fotos_paciente=[folde[2] for folde in os.walk(f'{path}/{paciente}/FLAIR')]\n    if len(fotos_paciente)==0:\n        continue\n    for foto in fotos_paciente[0]:\n        images_t.append(mpimg.imread(f\"{path}/{paciente}/FLAIR/{foto}\"))\n        pacientes_t.append(paciente)\n    \n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-29T23:12:08.058275Z","iopub.execute_input":"2022-10-29T23:12:08.058790Z","iopub.status.idle":"2022-10-29T23:12:23.194749Z","shell.execute_reply.started":"2022-10-29T23:12:08.058716Z","shell.execute_reply":"2022-10-29T23:12:23.193328Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Creo un df con el id del paciente y la fotos\n# (cada fila es una foto, un paciente tiene muchas filas por lo tanto)\n\n#TRAIN\nimages_df=pd.DataFrame()\nimages_df['Pacientes']=pacientes\nimages_df['Imagenes']=images\nimages_df\n\n#TEST\n","metadata":{"execution":{"iopub.status.busy":"2022-10-29T23:13:32.661182Z","iopub.execute_input":"2022-10-29T23:13:32.661826Z","iopub.status.idle":"2022-10-29T23:13:33.687555Z","shell.execute_reply.started":"2022-10-29T23:13:32.661777Z","shell.execute_reply":"2022-10-29T23:13:33.686601Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"     Pacientes                                           Imagenes\n0        00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n1        00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n2        00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n3        00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n4        00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n...        ...                                                ...\n1049     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n1050     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n1051     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n1052     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n1053     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...\n\n[1054 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pacientes</th>\n      <th>Imagenes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1049</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>1050</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>1051</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>1052</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n    <tr>\n      <th>1053</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1054 rows × 2 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# elimino todas las fotos que tengan un % blanco menor al 0.04, con esto eleminamos las fotos que son mayoritariamente negras\n\nimdf=pd.DataFrame(images_df['Imagenes'])\nimages_df['%Blanco']=imdf.applymap(lambda x: np.mean(x))['Imagenes']\nimages_df=images_df[images_df['%Blanco']>=0.04]\nimages_df=images_df.reset_index().drop(columns=['index'])\nimages_df\n\n# El df images_df esta listo para entrenar sobre el","metadata":{"execution":{"iopub.status.busy":"2022-10-29T23:14:10.597066Z","iopub.execute_input":"2022-10-29T23:14:10.598314Z","iopub.status.idle":"2022-10-29T23:14:11.946638Z","shell.execute_reply.started":"2022-10-29T23:14:10.598266Z","shell.execute_reply":"2022-10-29T23:14:11.945191Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"    Pacientes                                           Imagenes   %Blanco\n0       00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.091980\n1       00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.096151\n2       00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.068200\n3       00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.067668\n4       00688  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.069616\n..        ...                                                ...       ...\n809     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.136910\n810     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.159142\n811     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.196434\n812     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.170778\n813     00452  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  0.212120\n\n[814 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Pacientes</th>\n      <th>Imagenes</th>\n      <th>%Blanco</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.091980</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.096151</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.068200</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.067668</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>00688</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.069616</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>809</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.136910</td>\n    </tr>\n    <tr>\n      <th>810</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.159142</td>\n    </tr>\n    <tr>\n      <th>811</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.196434</td>\n    </tr>\n    <tr>\n      <th>812</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.170778</td>\n    </tr>\n    <tr>\n      <th>813</th>\n      <td>00452</td>\n      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n      <td>0.212120</td>\n    </tr>\n  </tbody>\n</table>\n<p>814 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import torch\n#Creo clase para facilitar la entrada de los datos a dataloaders\n\nclass BrainData(torch.utils.data.Dataset):\n    def __init__(self, dataframe, transforms):\n       \n     \n        self.transforms = transforms\n        self.dataFrame=dataframe\n        \n\n    def __getitem__(self, idx):\n         \n        #Unnecesary\n        #pacient_id =self.dataframe.loc[[idx], ['Pacientes']]\n        #Get image and category, the two statements that vit is going to use to learn\n        img=self.dataframe.loc[[idx],['Imagenes']]\n        #Cuando los labels esten en el df\n        category=self.dataframe.loc[[idx],['Labels']]\n        \n        if category=='0':\n            category=torch.Tensor([0.])\n        else:\n            category=torch.Tensor([1.])\n        \n        if self.transforms is not None:\n            img = self.transforms(img)\n\n        return img, category\n\n    def __len__(self):\n        return len(self.imgs)","metadata":{"execution":{"iopub.status.busy":"2022-10-29T23:53:15.886180Z","iopub.execute_input":"2022-10-29T23:53:15.887577Z","iopub.status.idle":"2022-10-29T23:53:16.737607Z","shell.execute_reply.started":"2022-10-29T23:53:15.887531Z","shell.execute_reply":"2022-10-29T23:53:16.736276Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Transforms: Main, the resize to the vit default size. Maybe we can append more???\n\nimport torchvision.transforms as T\nimport random\n\ndef get_transform(train):\n    transform = []\n    transform.append(T.PILToTensor())\n    transform.append(T.ConvertImageDtype(torch.float))\n    transform.append(T.Resize((224,224)))\n    if train:\n      #Optional to improve\n#         transform.append(T.RandomHorizontalFlip(0.5))\n#         transform.append(T.RandomAutocontrast())\n#         #hasta 30 deg\n#         deg=random.randint(1,30)\n#         transform.append(T.RandomRotation(degrees=deg))\n\n    return T.Compose(transform)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Creation of datasets (one to train, one to test)\n\ntrain_dataset=BrainData(images_df, get_transform(train=True))\ntest_dataset=BrainData(images_dfTest,get_transform(train=False))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Dataloaders\n\nimport torch.utils\n\n#data_loaders\ndata_loader = torch.utils.data.DataLoader(\n        train_dataset, batch_size=16, shuffle=True, num_workers=4)\n\ndata_loader_test = torch.utils.data.DataLoader(\n        test_dataset, batch_size=16, shuffle=False, num_workers=4)\n\ndataloaders={\"train\": data_loader, \"test\": data_loader_test}\n\ndataset_sizes={\"train\": len(train_dataset), \"test\":len(test_dataset)}","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Train model function\n\nimport time\nimport copy\nimport tqdm\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print(f'Epoch {epoch}/{num_epochs - 1}')\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'test']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n                scheduler.step()\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            #Tqdm for estimation of time\n            for inputs, labels in tqdm.tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n               \n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n               #Doesnt matter, only if we have time\n                #running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n        \n             #Doesnt matter, only if we have time\n            #epoch_loss = running_loss / dataset_sizes[phase]\n            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n            # Save best model\n            if phase == 'test' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n    print(f'Best test Acc: {best_acc:4f}')\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Define the parameters\n\nfrom torch import nn, optim\nfrom torch.optim import lr_scheduler\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n#Load VIT (Maybe a big vit model)\nmodel_transf = torchvision.models.vit_b_16(pretrained=True)\n\nfor param in model_transf.parameters():\n    param.requires_grad = False\n\n\n\n#Number of classes: 2, positive or negative. Access to the last layer and change output size\nmodel_transf.heads = nn.Linear(num_ftrs, 1)\n\n#loss function\ncriterion = nn.CrossEntropyLoss()\n#To be faster, change all params to only last layer param --->  model_transf.heads.head.in_features\noptimizer= optim.SGD(model_transf.parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#Train time!!\n\nprint(device)\nmodel_transf = train_model(model_transf, dataloaders, criterion, optimizer,scheduler num_epochs=25)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#With model trained, to predict:\n#Pseudo\n\n# input=imagen\n# output=model_transf(input)\n# _,pred=torch.max(output, 1)\n# return pred\n","metadata":{},"execution_count":null,"outputs":[]}]}